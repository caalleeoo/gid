import requests
from bs4 import BeautifulSoup
import csv
import json
import time
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import os

# --- 1. A Arte da Conex√£o (Resili√™ncia) ---
def configurar_sessao_robusta():
    """
    Cria uma sess√£o HTTP que n√£o desiste facilmente.
    Como um diplomata paciente, ela tenta renegociar se o servidor falhar.
    """
    sessao = requests.Session()
    retentativas = Retry(
        total=5,
        backoff_factor=2,  # Espera exponencial: 2s, 4s, 8s...
        status_forcelist=[429, 500, 502, 503, 504],
        raise_on_status=False
    )
    sessao.mount('https://', HTTPAdapter(max_retries=retentativas))
    return sessao

# --- 2. O N√∫cleo de Extra√ß√£o ---
def extrair_titulos_unb_massivo():
    # Configura√ß√µes de Arquivo
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M")
    arquivo_csv = f"riunb_titulos_{timestamp_str}.csv"
    arquivo_json = f"riunb_titulos_{timestamp_str}.json"
    
    # Par√¢metros da Jornada
    base_url = "https://repositorio.unb.br/browse"
    offset = 0
    rpp = 50  # 50 √© um n√∫mero √°ureo aqui: r√°pido o suficiente, leve o suficiente.
    total_coletado = 0
    
    # Lista para manter os dados em mem√≥ria para o JSON final
    buffer_memoria = []

    # Cabe√ßalhos para parecer um navegador leg√≠timo (√âtica Digital)
    headers = {
        'User-Agent': 'AcademicScraper/2.0 (University Research; contact: researcher@unb.br)',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }

    sessao = configurar_sessao_robusta()

    print(f"üèõÔ∏è  [IN√çCIO] Iniciando a Grande Coleta de T√≠tulos RIUnB")
    print(f"üì¶ [SA√çDA] CSV (Stream): {arquivo_csv} | JSON (Final): {arquivo_json}")
    print(f"üåä [LOTE] Processando {rpp} itens por p√°gina")
    print("-" * 60)

    try:
        # Abrimos o CSV em modo 'w' (write) e mantemos aberto
        with open(arquivo_csv, mode='w', newline='', encoding='utf-8-sig') as f_csv:
            writer = csv.writer(f_csv)
            # O cabe√ßalho agora inclui o Link, pois t√≠tulos sem link s√£o apenas texto morto.
            writer.writerow(['Titulo', 'Link_Relativo', 'Offset_Origem', 'Data_Coleta'])
            
            while True:
                # Ordena√ß√£o por t√≠tulo (ASC) garante a sequ√™ncia alfab√©tica
                params = {
                    'type': 'title', 
                    'order': 'ASC', 
                    'rpp': str(rpp), 
                    'offset': str(offset)
                }
                
                try:
                    # Feedback visual din√¢mico (sobrescreve a linha para n√£o poluir o terminal)
                    print(f"‚è≥ Coletando lote iniciando em {offset}... ", end='\r')
                    
                    response = sessao.get(base_url, params=params, headers=headers, timeout=60)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Estrat√©gia de Sele√ß√£o:
                    # O DSpace geralmente lista t√≠tulos dentro de <li> com class 'list-group-item'
                    # Dentro dele, procuramos o primeiro <a> que cont√©m o t√≠tulo.
                    itens = soup.find_all('li', class_='list-group-item')
                    
                    # Crit√©rio de Parada: P√°gina vazia
                    if not itens:
                        print(f"\n‚úÖ [FIM] O oceano de dados acabou. Offset final: {offset}.")
                        break

                    itens_nesta_pagina = 0
                    current_time = datetime.now().isoformat()

                    for item in itens:
                        tag_a = item.find('a')
                        
                        if tag_a:
                            titulo = tag_a.get_text(strip=True)
                            link = tag_a.get('href', '')
                            
                            # 1. Escreve no CSV (Seguran√ßa imediata)
                            writer.writerow([titulo, link, offset, current_time])
                            
                            # 2. Guarda na Mem√≥ria (Para o JSON final)
                            dado_estruturado = {
                                "titulo": titulo,
                                "url_relativa": link,
                                "origem_offset": offset,
                                "coletado_em": current_time
                            }
                            buffer_memoria.append(dado_estruturado)
                            itens_nesta_pagina += 1

                    # Valida√ß√£o de Seguran√ßa (Se a p√°gina retornou itens, mas n√£o conseguimos extrair nenhum)
                    if itens_nesta_pagina == 0 and len(itens) > 0:
                        print(f"\n‚ö†Ô∏è [ALERTA] Itens encontrados no HTML mas nenhum t√≠tulo extra√≠do. Verifique os seletores.")
                        # Opcional: break, mas melhor continuar para ver se √© s√≥ uma p√°gina ruim
                    
                    total_coletado += itens_nesta_pagina
                    print(f"‚ú® Progresso: {total_coletado} t√≠tulos salvos. (√öltimo lote: {itens_nesta_pagina} itens)")

                    offset += rpp
                    
                    # Pausa Elegante: N√£o bombardeie o servidor da universidade.
                    time.sleep(1.5) 

                except Exception as e:
                    print(f"\n‚ùå [ERRO] Falha no offset {offset}: {e}")
                    print(f"üõ°Ô∏è [RECUPERA√á√ÉO] O sistema aguardar√° 20s e tentar√° novamente...")
                    time.sleep(20)
                    # O 'continue' aqui faz o loop tentar o MESMO offset novamente
                    continue

        # --- Fase Final: A Cristaliza√ß√£o do JSON ---
        print("\n" + "-" * 60)
        print(f"üíæ Gerando arquivo JSON consolidado com {len(buffer_memoria)} registros...")
        
        with open(arquivo_json, 'w', encoding='utf-8') as f_json:
            json.dump(buffer_memoria, f_json, ensure_ascii=False, indent=4)

        print(f"üèÜ [SUCESSO ABSOLUTO]")
        print(f"   CSV: {os.path.abspath(arquivo_csv)}")
        print(f"   JSON: {os.path.abspath(arquivo_json)}")
        print(f"   Total de T√≠tulos: {total_coletado}")

    except KeyboardInterrupt:
        print(f"\nüõë [PAUSA MANUAL] Script interrompido. O CSV cont√©m dados seguros at√© o offset {offset}.")
        # Tenta salvar o JSON do que j√° temos
        if buffer_memoria:
            print("üíæ Salvando JSON parcial de emerg√™ncia...")
            with open(arquivo_json, 'w', encoding='utf-8') as f_json:
                json.dump(buffer_memoria, f_json, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    extrair_titulos_unb_massivo()