import pandas as pd
from rapidfuzz import process, fuzz
from unidecode import unidecode
import time
import sys
import os
from datetime import datetime

class IndexadorArtesanal:
    def __init__(self, caminho_arquivo, output_dir, threshold=85): # Aumentei levemente o threshold padr√£o
        self.caminho_arquivo = caminho_arquivo
        self.output_dir = output_dir
        self.threshold = threshold
        self.df_reduzido = None
        self.relatorio = []
        self.inicio = time.time()

    def normalizar(self, texto):
        """
        Normaliza√ß√£o leve: apenas min√∫sculas e strip.
        N√ÉO removemos acentos aqui para a visualiza√ß√£o final, 
        mas usaremos unidecode na compara√ß√£o interna se necess√°rio.
        """
        if not isinstance(texto, str):
            return str(texto)
        return texto.lower().strip()

    def carregar_e_agrupar(self):
        print(f"üìÇ [Fase 1] Carregamento Inteligente: {os.path.basename(self.caminho_arquivo)}")
        try:
            df_bruto = pd.read_csv(
                self.caminho_arquivo, 
                header=None, 
                sep=None, 
                engine='python',
                names=['Termo_Original', 'Frequencia_Raw'],
                dtype={0: str}
            )
            
            # Limpeza b√°sica
            df_bruto['Frequencia'] = pd.to_numeric(df_bruto['Frequencia_Raw'], errors='coerce').fillna(0)
            df_bruto['Termo_Original'] = df_bruto['Termo_Original'].fillna('')
            
            # Criamos uma chave normalizada para agrupar id√™nticos exatos primeiro (ex: "Casa " e "casa")
            df_bruto['Chave_Busca'] = df_bruto['Termo_Original'].apply(self.normalizar)

            # Agrupamento inicial (Otimiza√ß√£o)
            self.df_reduzido = df_bruto.groupby('Chave_Busca').agg({
                'Termo_Original': 'first', # Mant√©m a grafia original visualmente
                'Frequencia': 'sum'
            }).reset_index()
            
            # Ordena por frequ√™ncia (termos mais comuns costumam ser os "corretos")
            self.df_reduzido = self.df_reduzido.sort_values(by='Frequencia', ascending=False)

            print(f"   ‚Ü≥ Termos √∫nicos para an√°lise: {len(self.df_reduzido)}")
            print("-" * 50)

        except Exception as e:
            print(f"‚ùå Erro cr√≠tico ao ler arquivo: {e}")
            sys.exit(1)

    def analisar_profundidade(self):
        print("üß† [Fase 2] An√°lise Estrita (Grafia e Plurais)")
# --- 1. CL√ÅUSULA DE GUARDA (Seguran√ßa) ---
        # Se a madeira n√£o estiver na bancada, n√£o come√ßamos a trabalhar.
        if self.df_reduzido is None or self.df_reduzido.empty:
            print("‚ö†Ô∏è Aviso: 'df_reduzido' est√° vazio ou n√£o foi carregado. Opera√ß√£o cancelada.")
            return

        # --- 2. O C√ìDIGO ORIGINAL (Agora Seguro) ---
        print("   ‚Ü≥ Usando algoritmo 'Ratio' (considera o termo como um todo)")
        
        # Lista de termos para processar
        termos_processar = self.df_reduzido['Chave_Busca'].tolist()
        
        # Mapas para recupera√ß√£o r√°pida de dados originais
        mapa_original = pd.Series(
            self.df_reduzido.Termo_Original.values, 
            index=self.df_reduzido.Chave_Busca
        ).to_dict()
        
        mapa_frequencia = pd.Series(
            self.df_reduzido.Frequencia.values, 
            index=self.df_reduzido.Chave_Busca
        ).to_dict()
                
        ja_agrupados = set()
        total = len(termos_processar)
        
        # Vamos iterar. Como a lista est√° ordenada por frequ√™ncia, 
        # assumimos que o primeiro que aparece √© o "pai" (o termo correto/mais comum).
        for i, termo_pai in enumerate(termos_processar):
            
            if i % 100 == 0:
                percentual = (i / total) * 100
                sys.stdout.write(f"\r   ‚è≥ Progresso: {percentual:.1f}% ({i}/{total})")
                sys.stdout.flush()

            if termo_pai in ja_agrupados:
                continue
            
            # --- MUDAN√áA CRUCIAL AQUI ---
            # fuzz.ratio: Compara a string inteira. 
            # "Banana" vs "Bananas" = Alto
            # "Banana" vs "Banana Prata" = Baixo (diferen√ßa de tamanho penaliza)
            matches = process.extract(
                termo_pai, 
                termos_processar, 
                scorer=fuzz.ratio,  # <--- O SEGREDO EST√Å AQUI
                score_cutoff=self.threshold,
                limit=50
            )

            variacoes_encontradas = []
            freq_acumulada = 0
            
            # O primeiro match √© sempre ele mesmo (score 100), ent√£o verificamos se h√° outros
            if len(matches) > 1:
                
                for match in matches:
                    termo_filho = match[0]
                    score = match[1]
                    
                    # Ignora se j√° foi agrupado antes (a menos que seja o pr√≥prio pai desta rodada)
                    if termo_filho in ja_agrupados and termo_filho != termo_pai:
                        continue
                        
                    nome_display = mapa_original.get(termo_filho, termo_filho)
                    freq = mapa_frequencia.get(termo_filho, 0)
                    
                    # Formata a sa√≠da para voc√™ ver claramente a diferen√ßa
                    if termo_filho == termo_pai:
                        # O termo principal
                        freq_acumulada += freq
                        ja_agrupados.add(termo_pai)
                    else:
                        # As varia√ß√µes (filhos)
                        variacoes_encontradas.append(f"{nome_display} [Score: {score:.0f}]")
                        freq_acumulada += freq
                        ja_agrupados.add(termo_filho)
                
                # S√≥ adiciona ao relat√≥rio se encontrou FILHOS (varia√ß√µes reais)
                if variacoes_encontradas:
                    self.relatorio.append({
                        'Termo Sugerido (Mais Comum)': mapa_original[termo_pai],
                        'Varia√ß√µes Detectadas (Duplicatas)': " | ".join(variacoes_encontradas),
                        'Qtd Varia√ß√µes': len(variacoes_encontradas),
                        'Frequ√™ncia Total': int(freq_acumulada)
                    })

        tempo = (time.time() - self.inicio) / 60
        print(f"\n\n‚úÖ An√°lise conclu√≠da em {tempo:.2f} minutos.")
        print(f"üìä {len(self.relatorio)} grupos de corre√ß√µes encontrados.")

    def salvar(self):
        if not self.relatorio:
            print("‚ú® Nenhuma duplicata encontrada com esses par√¢metros.")
            return

        df_final = pd.DataFrame(self.relatorio)
        # Ordena para mostrar os casos com mais varia√ß√µes primeiro (onde est√° a maior sujeira)
        df_final = df_final.sort_values(by='Qtd Varia√ß√µes', ascending=False)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        nome_arquivo = f"relatorio_correcao_fina_{timestamp}.csv"
        caminho_completo = os.path.join(self.output_dir, nome_arquivo)
        
        df_final.to_csv(caminho_completo, index=False, sep=';', encoding='utf-8-sig')
        print(f"üíæ Relat√≥rio salvo em: {caminho_completo}")

if __name__ == "__main__":
    # --- CONFIGURA√á√ÉO ---
    dir_script = os.path.dirname(os.path.abspath(__file__))
    dir_raiz = os.path.abspath(os.path.join(dir_script, '..', '..'))
    
    # 1. Ajuste o nome do arquivo aqui
    NOME_ARQUIVO_ALVO = 'assuntos.csv' 
    
    # Busca autom√°tica
    caminho_input = os.path.join(dir_raiz, 'data', 'temp', NOME_ARQUIVO_ALVO)
    if not os.path.exists(caminho_input):
         caminho_input = os.path.join(dir_raiz, 'data', 'raw', NOME_ARQUIVO_ALVO)

    dir_output = os.path.join(dir_raiz, 'data', 'processed')
    os.makedirs(dir_output, exist_ok=True)
    
    # 2. Sensibilidade (Sugest√£o: 85 ou 88 para pegar apenas typos e plurais)
    # Se colocar 95 pega apenas typos muito √≥bvios.
    # Se colocar 70 come√ßa a pegar coisas erradas.
    SENSIBILIDADE = 88 
    
    print("="*60)
    print(f"üîç DETETIVE DE GRAFIA E PLURAIS - UnB")
    print("="*60)
    
    if os.path.exists(caminho_input):
        app = IndexadorArtesanal(caminho_input, dir_output, threshold=SENSIBILIDADE)
        app.carregar_e_agrupar()
        app.analisar_profundidade()
        app.salvar()
    else:
        print(f"‚ùå ARQUIVO N√ÉO ENCONTRADO: {NOME_ARQUIVO_ALVO}")